# LightRAG ATS - Inner Architecture

## ðŸ§  Techniques & Algorithms Used

### Stage 1: Resume Ingestion

| Step                    | Technique                   | Implementation                                         |
| ----------------------- | --------------------------- | ------------------------------------------------------ |
| **Text Extraction**     | Rule-based parsing          | `PyMuPDF` (PDF), `python-docx` (DOCX)                  |
| **Chunking**            | Token-based splitting       | Split at ~1200 tokens with 100 overlap                 |
| **Embedding**           | Dense Vector Encoding       | `BAAI/bge-m3` â†’ 1024-dimensional vectors               |
| **Entity Extraction**   | LLM Prompting               | Ollama `qwen2.5:7b` extracts: skills, roles, companies |
| **Relation Extraction** | LLM Prompting               | "Person â†’ WORKED_AT â†’ Company" triples                 |
| **Storage**             | Vector DB + Knowledge Graph | NanoVectorDB + NetworkX                                |

---

### Stage 2: Candidate Filtering (`/analyze`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VECTOR SIMILARITY SEARCH                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Technique: Cosine Similarity on Dense Embeddings                â”‚
â”‚                                                                  â”‚
â”‚  1. Job Description â†’ Embed with bge-m3 â†’ Query Vector (1024d)  â”‚
â”‚  2. Compare with stored resume chunks using cosine similarity    â”‚
â”‚  3. Return top-k most similar chunks                             â”‚
â”‚                                                                  â”‚
â”‚  Formula: similarity = cos(query_vec, doc_vec)                   â”‚
â”‚           = (q Â· d) / (||q|| Ã— ||d||)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Algorithm:** Approximate Nearest Neighbor (ANN) via NanoVectorDB
**Metric:** Cosine similarity (range: -1 to 1, higher = more similar)

---

### Stage 3: Chat Query (`/chat/query`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RETRIEVAL-AUGMENTED GENERATION (RAG)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   RETRIEVAL  â”‚ â”€â”€â†’ â”‚   CONTEXT    â”‚ â”€â”€â†’ â”‚  GENERATION  â”‚     â”‚
â”‚  â”‚              â”‚     â”‚  ASSEMBLY    â”‚     â”‚              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                  â”‚
â”‚  Mode: naive   â†’ Vector search only (fastest)                   â”‚
â”‚  Mode: local   â†’ Entity-specific from Knowledge Graph           â”‚
â”‚  Mode: global  â†’ Relationship patterns from KG                  â”‚
â”‚  Mode: hybrid  â†’ local + global combined                        â”‚
â”‚  Mode: mix     â†’ Vector + Graph (dual-level, recommended)       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Retrieval Modes Explained:**

| Mode       | What It Does                                   | Best For                        |
| ---------- | ---------------------------------------------- | ------------------------------- |
| **naive**  | Pure vector similarity search                  | Fast lookups, simple queries    |
| **local**  | Finds specific entities (skills, people) in KG | "Who knows Python?"             |
| **global** | Analyzes relationship patterns                 | "What companies work with AWS?" |
| **hybrid** | Combines local + global                        | Complex analysis                |
| **mix**    | Vector results + Graph context                 | Best overall (default)          |

---

### Stage 4: Response Generation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM RESPONSE GENERATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Model: Ollama qwen2.5:7b (7 billion parameters)                â”‚
â”‚  Type: Instruction-tuned decoder-only transformer               â”‚
â”‚                                                                  â”‚
â”‚  Prompt Structure:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SYSTEM: You are an ATS assistant...                       â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚ CONTEXT: [Retrieved chunks + KG entities/relations]       â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚ USER: {query}                                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  Output: Natural language answer grounded in context            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary: Techniques by Stage

| Stage                 | Primary Technique     | Model/Algorithm        |
| --------------------- | --------------------- | ---------------------- |
| **Ingestion**         | Chunking + Embedding  | bge-m3 (1024d vectors) |
| **Entity Extraction** | LLM Prompting         | qwen2.5:7b             |
| **Filtering**         | Vector Similarity     | Cosine, ANN search     |
| **Graph Retrieval**   | Knowledge Graph Query | NetworkX traversal     |
| **Response**          | RAG + LLM Generation  | qwen2.5:7b             |

---

â”œâ”€â”€ api/ # FastAPI Application Layer
â”‚ â”œâ”€â”€ main.py # App entry, lifespan, health endpoints
â”‚ â”œâ”€â”€ models.py # Pydantic request/response models
â”‚ â”œâ”€â”€ middleware.py # Logging, error handling
â”‚ â””â”€â”€ routes/
â”‚ â”œâ”€â”€ analyze.py # POST /analyze - candidate ranking
â”‚ â”œâ”€â”€ chat.py # POST /chat/_ - RAG queries
â”‚ â””â”€â”€ ingest.py # POST /ingest - resume upload
â”‚
â”œâ”€â”€ src/ # Core Business Logic
â”‚ â”œâ”€â”€ config.py # Settings from .env
â”‚ â”œâ”€â”€ rag*config.py # LightRAG initialization
â”‚ â”œâ”€â”€ llm_adapter.py # Ollama API wrapper
â”‚ â”œâ”€â”€ embedding.py # BAAI/bge-m3 embeddings
â”‚ â”œâ”€â”€ ingestion.py # Resume processing pipeline
â”‚ â”œâ”€â”€ resume_parser.py # PDF/DOCX/TXT extraction
â”‚ â”œâ”€â”€ dual_retrieval.py # Vector + Graph retrieval
â”‚ â””â”€â”€ reranker.py # Cross-encoder reranking
â”‚
â”œâ”€â”€ rag_storage/ # Persisted Data (JSON files)
â”‚ â”œâ”€â”€ vdb_entities.json # Entity embeddings
â”‚ â”œâ”€â”€ vdb_relationships.json # Relation embeddings
â”‚ â”œâ”€â”€ vdb_chunks.json # Document chunk embeddings
â”‚ â”œâ”€â”€ kv_store*_.json # Key-value stores
â”‚ â””â”€â”€ graph\_\*.graphml # NetworkX graph
â”‚
â””â”€â”€ scripts/ # CLI Tools
â”œâ”€â”€ ingest_resumes.py # Batch ingestion
â””â”€â”€ test_query.py # Query testing

````

---

## Component Architecture

```mermaid
classDiagram
    class FastAPI {
        +lifespan()
        +health_check()
        +get_stats()
    }

    class AnalyzeRouter {
        +analyze_job(request)
        +get_job_analysis(job_id)
        -parse_candidates_from_response()
    }

    class ChatRouter {
        +chat_about_job(request)
        +direct_query(request)
    }

    class RAGManager {
        -_rag: LightRAG
        -_initialized: bool
        +initialize()
        +close()
        +rag: LightRAG
    }

    class LightRAG {
        +ainsert(text)
        +aquery(query, param)
        +initialize_storages()
        -full_docs
        -text_chunks
        -entities_vdb
        -relationships_vdb
        -chunk_entity_relation_graph
    }

    class OllamaAdapter {
        +generate(prompt)
        +check_health()
        -_client: httpx.AsyncClient
    }

    class EmbeddingModel {
        +encode(texts)
        +aencode(texts)
        -model: SentenceTransformer
    }

    class DualLevelRetrieval {
        +retrieve(query, mode)
        -_vector_search()
        -_graph_search()
    }

    FastAPI --> AnalyzeRouter
    FastAPI --> ChatRouter
    AnalyzeRouter --> RAGManager
    ChatRouter --> RAGManager
    ChatRouter --> DualLevelRetrieval
    RAGManager --> LightRAG
    LightRAG --> OllamaAdapter
    LightRAG --> EmbeddingModel
    DualLevelRetrieval --> LightRAG
````

---

## Data Flow Inside LightRAG

```mermaid
flowchart LR
    subgraph ainsert["ainsert() - Document Ingestion"]
        A1[Input Text] --> A2[Chunking<br/>~1200 tokens]
        A2 --> A3[Embed Chunks]
        A3 --> A4[Store in VectorDB]
        A2 --> A5[LLM: Extract Entities]
        A5 --> A6[Build Knowledge Graph]
    end

    subgraph aquery["aquery() - Retrieval"]
        Q1[Query] --> Q2[Embed Query]
        Q2 --> Q3{Mode?}
        Q3 -->|naive| Q4[Vector Search Only]
        Q3 -->|local| Q5[Entity Retrieval]
        Q3 -->|global| Q6[Relationship Retrieval]
        Q3 -->|mix| Q7[Vector + Graph]
        Q4 & Q5 & Q6 & Q7 --> Q8[Build Context]
        Q8 --> Q9[LLM Generate Response]
    end
```

---

## Key Classes

### `RAGManager` (src/rag_config.py)

```python
class RAGManager:
    """Singleton managing LightRAG lifecycle"""

    async def initialize(self):
        # 1. Create LightRAG instance
        # 2. Configure LLM (Ollama)
        # 3. Configure embeddings (bge-m3)
        # 4. Call initialize_storages() â† CRITICAL!

    @property
    def rag(self) -> LightRAG:
        # Returns initialized LightRAG instance
```

### `OllamaAdapter` (src/llm_adapter.py)

```python
class OllamaAdapter:
    """Async wrapper for Ollama API"""

    async def generate(prompt, system_prompt, **kwargs):
        # POST to http://localhost:11434/api/chat
        # Returns: Generated text
```

### `DualLevelRetrieval` (src/dual_retrieval.py)

```python
class DualLevelRetrieval:
    """Combines vector and graph retrieval"""

    async def retrieve(query, candidates, mode):
        # 1. Try requested mode
        # 2. Fallback to simpler modes on error
        # 3. Inject candidate context
        # 4. Return LLM response
```

---

## Storage Internals

| File                                  | Contents                  | Format                          |
| ------------------------------------- | ------------------------- | ------------------------------- |
| `vdb_entities.json`                   | Entity embeddings         | `{id: [1024-dim vector]}`       |
| `vdb_chunks.json`                     | Document chunk embeddings | `{chunk_id: [vector]}`          |
| `kv_store_full_docs.json`             | Full document text        | `{doc_id: {content: "..."}}`    |
| `kv_store_text_chunks.json`           | Chunked text              | `{chunk_id: {content, tokens}}` |
| `graph_chunk_entity_relation.graphml` | Knowledge graph           | NetworkX GraphML                |

---

## Request Processing

### POST /analyze

```
Request â†’ analyze.py
    â†“
get_rag() â†’ RAGManager.rag
    â†“
rag.aquery(job_description, mode="naive")
    â†“
LightRAG:
    1. Embed query
    2. Search vdb_chunks.json
    3. Return top-k similar chunks
    â†“
parse_candidates_from_response()
    â†“
Response: [{name, score, highlights}]
```

### POST /chat/query

```
Request â†’ chat.py
    â†“
chat_with_dual_retrieval()
    â†“
DualLevelRetrieval.retrieve(mode="mix")
    â†“
LightRAG.aquery() with fallback chain:
    mix â†’ hybrid â†’ local â†’ naive
    â†“
Response: {response, mode_used, sources}
```
